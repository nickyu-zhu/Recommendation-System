{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fea6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from transformers import *\n",
    "import progressbar\n",
    "import sys\n",
    "#sys.path.append('train.py')\n",
    "#from train import Linear, Gaussian, load_data, Square\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90648513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recipe data\n",
    "with open(\"RAW_recipes.csv\", 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    raw_read = list(reader)\n",
    "\n",
    "# Get the ingredients for each recipe and associate them with the recipe's unique ID\n",
    "id_to_ing = dict()\n",
    "id_to_rating = dict()\n",
    "for i,recipe in enumerate(raw_read[1:]):\n",
    "    # Remove junk characters from ingredients\n",
    "    id_to_ing[recipe[1]] = [a.strip(\"'\").strip('\"') for a in recipe[10].strip('\"[').strip(']\"').split(\", \")]\n",
    "    id_to_rating[recipe[1]] = []\n",
    "\n",
    "del raw_read\n",
    "\n",
    "# Find ingredients that appear in fewer than five recipes\n",
    "wordCounter = collections.Counter()\n",
    "for id in id_to_ing:\n",
    "    for word in set(id_to_ing[id]):\n",
    "        wordCounter[word] += 1\n",
    "for key in wordCounter:\n",
    "    if wordCounter[key] < 5:\n",
    "        wordCounter[key] = -1\n",
    "wordCounter = +wordCounter\n",
    "goodWords = set(wordCounter.keys())\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "# Delete ingredients that appear in fewer than five recipes\n",
    "for id in id_to_ing:\n",
    "    id_to_ing[id] = [a for a in id_to_ing[id] if a in goodWords]\n",
    "    if id_to_ing == []:\n",
    "        to_delete += [id]\n",
    "for item in to_delete:\n",
    "    del id_to_rating[item]\n",
    "\n",
    "# Load the user interaction data\n",
    "with open(\"RAW_interactions.csv\", 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    raw_ratings = list(reader)\n",
    "\n",
    "# Get the ratings for ecah recipe\n",
    "for i, interaction in enumerate(raw_ratings[1:]):\n",
    "    if interaction[1] in id_to_rating.keys() and interaction[3] != \"0\":\n",
    "        id_to_rating[interaction[1]] += [int(interaction[3])]\n",
    "\n",
    "del raw_ratings\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "# Delete recipes that have no rating\n",
    "for recipe in id_to_rating:\n",
    "    if id_to_rating[recipe] == []:\n",
    "        to_delete += [recipe]\n",
    "    else:\n",
    "        id_to_rating[recipe] = sum(id_to_rating[recipe])/len(id_to_rating[recipe])\n",
    "for item in to_delete:\n",
    "    del id_to_rating[item]\n",
    "\n",
    "\n",
    "# Get rid of the recipe ID, instead associating the ingredients with the rating\n",
    "X = [0 for i in range(len(id_to_rating))]\n",
    "Y = [0 for i in range(len(id_to_rating))]\n",
    "for i, id in enumerate(id_to_rating.keys()):\n",
    "    X[i] = id_to_ing[id]\n",
    "    Y[i] = id_to_rating[id]\n",
    "\n",
    "# Save as a training set, valdiation set, and test set (70/15/15 split)\n",
    "Y = np.array(Y)\n",
    "with open('X_train.txt', 'w') as f:\n",
    "    for ingredients in X[:158612]:\n",
    "        for ing in ingredients:\n",
    "            f.write(ing+'#')\n",
    "        f.write('\\n')\n",
    "with open('X_val.txt', 'w') as f:\n",
    "    for ingredients in X[158612:192601]:\n",
    "        for ing in ingredients:\n",
    "            f.write(ing+'#')\n",
    "        f.write('\\n')\n",
    "with open('X_test.txt', 'w') as f:\n",
    "    for ingredients in X[192601:]:\n",
    "        for ing in ingredients:\n",
    "            f.write(ing+'#')\n",
    "        f.write('\\n')\n",
    "np.save('Y_train',Y[:158612])\n",
    "np.save('Y_val',Y[158612:192601])\n",
    "np.save('Y_test',Y[192601:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7159c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(predictions, Y):\n",
    "    \"\"\" Return the MSE loss. \"\"\"\n",
    "    return 0.5 * np.linalg.norm(Y - predictions)**2 /len(Y)\n",
    "\n",
    "def avg_dif(predictions, Y):\n",
    "    \"\"\" Calculate the average L1 distance between predictions and true ratings. \"\"\"\n",
    "    return sum([abs(Y[i] - predictions[i]) for i in range(len(Y))])[0] / len(Y)\n",
    "\n",
    "def lin_reg_prediction(X, theta):\n",
    "    \"\"\" Return the prediction vector for inputs X using theta output from linear regression. \"\"\"\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def kernel_prediction(beta, K):\n",
    "    \"\"\" Return the prediction vector for input kernel K using beta output from kernel methods. \"\"\"\n",
    "    return np.dot(beta.T, K).T\n",
    "\n",
    "def calc_grad(X,Y,theta):\n",
    "    \"\"\" Calculate the gradient for use in gradient descent for linear regression. \"\"\"\n",
    "    return np.dot((Y - np.dot(X,theta)).T,X).T / (X.shape[0])\n",
    "\n",
    "def linear_regression(data, embedding, iters, learning_rate = 0.1, makePlot = True):\n",
    "    \"\"\" Perform linear regression. \"\"\"\n",
    "    (X, X_val, X_test, Y, Y_val, Y_test) = data\n",
    "    theta = np.zeros((X.shape[1],1))\n",
    "\n",
    "    tloss = []\n",
    "    vloss = []\n",
    "\n",
    "    for i in progressbar.progressbar(iters):\n",
    "        theta += learning_rate * calc_grad(X, Y, theta)\n",
    "\n",
    "        tloss += [loss(lin_reg_prediction(X, theta), Y)]\n",
    "        vloss += [loss(lin_reg_prediction(X_val, theta), Y_val)]\n",
    "    if makePlot:\n",
    "        plot(iters, tloss, vloss, embedding, \"Linear Regression\")\n",
    "    test_predictions = lin_reg_prediction(X_test, theta)\n",
    "    print(\"Average difference in star rating: %.4f\" %avg_dif(test_predictions, Y_test))\n",
    "    return tloss, vloss, loss(test_predictions, Y_test)\n",
    "\n",
    "def Linear(xi, xj):\n",
    "    \"\"\" Linear kernel function. \"\"\"\n",
    "    return np.dot(xi,xj.T)\n",
    "\n",
    "def Square(xi, xj):\n",
    "    \"\"\" Square kernel function. \"\"\"\n",
    "    return np.dot(xi,xj.T)**2\n",
    "\n",
    "def Gaussian(xi, xj, sigma = 1):\n",
    "    \"\"\" Gaussian kernel function. \"\"\"\n",
    "    return np.exp( - np.dot((xi-xj), (xi-xj).T) / (2 * sigma**2))\n",
    "\n",
    "def kernel_method(data, kernel, embedding,  iters, learning_rate = 0.0001, makePlot = True):\n",
    "    \"\"\" Use kernel method to learn parameter vector beta for prediction. \"\"\"\n",
    "    (K_train, K_val, K_test, Y_train, Y_val, Y_test) = data\n",
    "    beta = np.zeros((K_train.shape[0],1))\n",
    "\n",
    "    tloss = []\n",
    "    vloss = []\n",
    "\n",
    "    for i in progressbar.progressbar(iters):\n",
    "        beta += learning_rate * ( Y_train - np.dot(beta.T, K_train).T )\n",
    "        tloss += [loss(kernel_prediction(beta, K_train), Y_train)]\n",
    "        vloss += [loss(kernel_prediction(beta, K_val), Y_val)]\n",
    "\n",
    "    if makePlot:\n",
    "        plot(iters, tloss, vloss, embedding, kernel.__name__+\" Kernel\")\n",
    "    test_predictions = kernel_prediction(beta, K_test)\n",
    "    print(\"Average difference in star rating: %.4f\" %avg_dif(test_predictions, Y_test))\n",
    "    return tloss, vloss, loss(test_predictions, Y_test)\n",
    "\n",
    "def plot(iters, tloss, vloss, embedding, method):\n",
    "    \"\"\" Make a plot of training and validation loss during training. \"\"\"\n",
    "    plt.plot(iters,tloss,label='training loss')\n",
    "    plt.plot(iters,vloss,label='validation loss')\n",
    "\n",
    "    plt.title('Average Loss per Example During Training for \\n'+embedding+' Embedding with '+method)\n",
    "    plt.xlabel('iteration number')\n",
    "    plt.ylabel('average loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def learning_rate_sweep(vals_to_test, func):\n",
    "    \"\"\" Meta-function to sweep over possible learning rates and plot validation set loss. \"\"\"\n",
    "    outputs = []\n",
    "    for i in vals_to_test:\n",
    "        _, vloss, _ = func(i)\n",
    "        outputs += [vloss[-1]]\n",
    "    plt.plot(vals_to_test, outputs, 'o--')\n",
    "    plt.title('Average Loss over Validation Set')\n",
    "    plt.xlabel('learning rate')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    print(outputs)\n",
    "\n",
    "def load_data(embedding, trim = True):\n",
    "    \"\"\" Load features as saved by features_X.py. \"\"\"\n",
    "    Y_train = np.expand_dims(np.load('Y_train.npy'), axis=1)\n",
    "    Y_val = np.expand_dims(np.load('Y_val.npy'), axis=1)\n",
    "    Y_test = np.expand_dims(np.load('Y_test.npy'), axis=1)\n",
    "\n",
    "    if embedding == \"One-Hot\":\n",
    "        X_train = sparse.load_npz('X_train_one_hot.npz').todense()\n",
    "        X_val = sparse.load_npz('X_val_one_hot.npz').todense()\n",
    "        X_test = sparse.load_npz('X_test_one_hot.npz').todense()\n",
    "\n",
    "    if embedding == \"BERT\":\n",
    "        X_train = np.load('X_train_bert.npy')\n",
    "        X_val = np.load('X_val_bert.npy')\n",
    "        X_test = np.load('X_test_bert.npy')\n",
    "\n",
    "    if trim:\n",
    "        X_train = X_train[:5000,:]\n",
    "        X_val = X_val[:1000,:]\n",
    "        X_test = X_test[:1000,:]\n",
    "\n",
    "        Y_train = Y_train[:5000,:]\n",
    "        Y_val = Y_val[:1000,:]\n",
    "        Y_test = Y_test[:1000,:]\n",
    "\n",
    "    return (X_train, X_val, X_test, Y_train, Y_val, Y_test)\n",
    "\n",
    "def load_kernel(embedding, kernel):\n",
    "    \"\"\" Load kernel matrix as saved by features_X.py. \"\"\"\n",
    "    Y_train = np.expand_dims(np.load('Y_train.npy'), axis=1)[:5000,:]\n",
    "    Y_val = np.expand_dims(np.load('Y_val.npy'), axis=1)[:1000,:]\n",
    "    Y_test = np.expand_dims(np.load('Y_test.npy'), axis=1)[:1000,:]\n",
    "\n",
    "    K_train = np.load('K_train_'+embedding+'_'+kernel.__name__+'.npy')\n",
    "    K_val = np.load('K_val_'+embedding+'_'+kernel.__name__+'.npy')\n",
    "    K_test = np.load('K_test_'+embedding+'_'+kernel.__name__+'.npy')\n",
    "\n",
    "    return (K_train, K_val, K_test, Y_train, Y_val, Y_test)\n",
    "\n",
    "def pretty_print(inps):\n",
    "    \"\"\" Print out training, validation, and test set losses. \"\"\"\n",
    "    train_loss, validation_loss, test_loss = inps\n",
    "    print(\"Training loss: %.4f   Validation loss: %.4f   Test loss: %.4f\" %(train_loss[-1], validation_loss[-1], test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064dc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw(filename, size):\n",
    "    \"\"\" Load the text files output by the clean_data.py script. \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        X = [a.rstrip('#\\n').split('#') for a in list(f)]\n",
    "    return X[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b717438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(X_train, X_val, X_test):\n",
    "    \"\"\" Create a set of the unique ingredients. \"\"\"\n",
    "    vocab = set()\n",
    "    for X in [X_train, X_val, X_test]:\n",
    "        for ing_list in X:\n",
    "            for ing in ing_list:\n",
    "                vocab.add(ing)\n",
    "    vocab = list(vocab)\n",
    "    print(len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b440e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(X, vocab, filename):\n",
    "    \"\"\" Save the design matrix with a one-hot encoding. \"\"\"\n",
    "    X_one_hot = np.append(np.ones((len(X), 1)), np.zeros((len(X), len(vocab))),axis=1)\n",
    "    for i,recipe in enumerate(X):\n",
    "        for ing in recipe:\n",
    "            X_one_hot[i, vocab.index(ing)] = 1\n",
    "\n",
    "    X_one_hot = sparse.coo_matrix(X_one_hot)\n",
    "    sparse.save_npz(filename, X_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64793bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert(X, filename):\n",
    "    \"\"\" Save the design matrix with BERT embedding. \"\"\"\n",
    "    X_out = np.zeros((len(X),768))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        X_preprocess = [\"[CLS] \" + ' ; '.join(x)+\" [SEP]\" for x in X]\n",
    "        tokens = [tokenizer.tokenize(x) for x in X_preprocess]\n",
    "        indexed_tokens = [tokenizer.convert_tokens_to_ids(recipe) for recipe in tokens]\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        model.eval()\n",
    "        for i, recipe_embedding in enumerate(indexed_tokens):\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            tokens_tensor = torch.tensor([recipe_embedding])\n",
    "            segments_tensor = torch.ones_like(tokens_tensor)\n",
    "            encoded_layers, _ = model(tokens_tensor, segments_tensor)\n",
    "\n",
    "            X_out[i, :] = np.array(torch.mean(encoded_layers[0], dim=0))\n",
    "    np.save(filename, X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c0fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel(X_train, X_test, kernel_func, symmetric = False):\n",
    "    \"\"\" Create the kernel matrix. Use symmetric to speed up computation if X_train and X_test are identical. \"\"\"\n",
    "    K = np.zeros((X_train.shape[0], X_test.shape[0]))\n",
    "    if symmetric:\n",
    "        for i in progressbar.progressbar(range(X_train.shape[0])):\n",
    "            for j in range(i+1):\n",
    "                K[i,j] = K[j,i] = kernel_func(X_train[i,:], X_test[j,:])\n",
    "    else:\n",
    "        for i in progressbar.progressbar(range(X_train.shape[0])):\n",
    "            for j in range(X_test.shape[0]):\n",
    "                K[i,j] = kernel_func(X_train[i,:], X_test[j,:])\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8858cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelize(embedding, kernel):\n",
    "    \"\"\" Create the kernel matrices for the training, validation, and test sets. \"\"\"\n",
    "    (X_train, X_val, X_test, Y_train, Y_val, Y_test) = load_data(embedding)\n",
    "    np.save('K_train_'+embedding+'_'+kernel.__name__, get_kernel(X_train, X_train, kernel, symmetric = True))\n",
    "    np.save('K_val_'+embedding+'_'+kernel.__name__, get_kernel(X_train, X_val, kernel, symmetric = False))\n",
    "    np.save('K_test_'+embedding+'_'+kernel.__name__, get_kernel(X_train, X_test, kernel, symmetric = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9475fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/yizhechen/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/yizhechen/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/yizhechen/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/yizhechen/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/yizhechen/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\" Load the outputs of clean_data and build and save the feature vectors and kernels used in training. \"\"\"\n",
    "    X_train = load_raw('X_train.txt', 5000)\n",
    "    X_val = load_raw('X_train.txt', 1000)\n",
    "    X_test = load_raw('X_train.txt', 1000)\n",
    "\n",
    "    vocab = make_vocab(X_train, X_val, X_test)\n",
    "\n",
    "    one_hot(X_train, vocab, 'X_train_one_hot')\n",
    "    one_hot(X_val, vocab, 'X_val_one_hot')\n",
    "    one_hot(X_test, vocab, 'X_test_one_hot')\n",
    "\n",
    "    bert(X_train, 'X_train_bert')\n",
    "    bert(X_val, 'X_val_bert')\n",
    "    bert(X_test, 'X_test_bert')\n",
    "\n",
    "    kernelize(\"One-Hot\", Square)\n",
    "    kernelize(\"One-Hot\", Gaussian)\n",
    "    kernelize(\"BERT\", Square)\n",
    "    kernelize(\"BERT\", Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f36f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/y1b9519s455gxzhm12ll2sm80000gn/T/ipykernel_40098/1376764450.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One-Hot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"One-Hot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One-Hot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"One-Hot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dk/y1b9519s455gxzhm12ll2sm80000gn/T/ipykernel_40098/1264614046.py\u001b[0m in \u001b[0;36mlinear_regression\u001b[0;34m(data, embedding, iters, learning_rate, makePlot)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mvloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogressbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcalc_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pretty_print(linear_regression(load_data(\"One-Hot\"), \"One-Hot\", range(50), 0.5))\n",
    "\n",
    "    pretty_print(kernel_method(load_kernel(\"One-Hot\", Linear), Linear, \"One-Hot\", range(50), 0.0002))\n",
    "\n",
    "    pretty_print(kernel_method(load_kernel(\"One-Hot\", Gaussian), Gaussian, \"One-Hot\", range(100), 0.015))\n",
    "\n",
    "    pretty_print(linear_regression(load_data(\"BERT\"), \"BERT\", range(100), 0.02))\n",
    "\n",
    "    pretty_print(kernel_method(load_kernel(\"BERT\", Linear), Linear, \"BERT\", range(100), 4*10**-6))\n",
    "\n",
    "    pretty_print(kernel_method(load_kernel(\"BERT\", Gaussian), Gaussian, \"BERT\", range(100), 0.05))\n",
    "\n",
    "    pretty_print(kernel_method(load_kernel(\"One-Hot\", Square), Square, \"One-Hot\", range(100), 0.5*10**-4))\n",
    "\n",
    "    pretty_print(kernel_method(load_kernel(\"BERT\", Square), Square, \"BERT\", range(100), 3*10**-8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
